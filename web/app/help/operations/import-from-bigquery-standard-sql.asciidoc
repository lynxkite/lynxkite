### Import from BigQuery (Standard SQL result)

Execute a BigQuery Standard SQL query and get the result via Storage API.
Some BigQuery-specific datatypes like ARRAY<STRUCT> may not be fully supported within LynxKite
More information here: https://github.com/GoogleCloudDataproc/spark-bigquery-connector/tree/0.25.0

====
[p-parent_project_id]#GCP project ID for billing#::
The GCP Project ID for billing purposes. This may be different from the dataset being read.
`roles/bigquery.readSessionUser` and `roles/bigquery.jobUser` are both required on the billed project.

[p-materialization_project_id]#GCP project for materialization#::
Used together with dataset ID. Defaults to service account's GCP project if unspecified.
Requires `roles/bigquery.dataEditor` on the dataset.

[p-materialization_dataset_id]#BigQuery Dataset for materialization#::
Place to create temporary (24h) tables to store the results of the SQL query.
Dataset must already exist with `roles/bigquery.dataEditor` granted on it.

[p-bq_standard_sql]#BigQuery Standard SQL#::
The SQL query to run on BigQuery. `roles/bigquery.dataViewer` is required for SELECT.

[p-credentials_file]#Path to Service Account JSON key#::
Local path to the service account JSON key to authenticate with GCP.
Leave this field empty if running on Dataproc or to use `GOOGLE_APPLICATION_CREDENTIALS`.

include::{g}[tag=import-box]
====
