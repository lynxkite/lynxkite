### Train a decision tree regression model

Trains a decision tree regression model using the graph's vertex attributes.
The algorithm recursively partitions the feature space into two parts. The tree
predicts the same label for each bottommost (leaf) partition. Each binary
partitioning is chosen from a set of possible splits in order to maximize the
information gain at the corresponding tree node. For calculating the information
gain the variance of the nodes is used:
the information gain is the difference between the parent node variance and the
weighted sum of the two child node variances.
https://spark.apache.org/docs/latest/mllib-decision-tree.html#basic-algorithm[More information about the parameters.]

Note: Once the tree is trained there is only a finite number of possible predictions.
Because of this, the regression model might seem like a classification. The main
difference is that these buckets ("classes") are invented by the algorithm during
the training in order to minimize the variance.

====
[p-name]#Model name#::
The model will be stored as a graph attribute using this name.

[p-label]#Label attribute#::
The vertex attribute the model is trained to predict.

[p-features]#Feature attributes#::
The attributes the model learns to use for making predictions.

[p-maxbins]#Maximum number of bins#::
Number of bins used when discretizing continuous features.

[p-maxdepth]#Maximum depth#::
Maximum depth of the tree.

[p-mininfogain]#Minimum information gain#::
Minimum information gain for a split to be considered as a tree node.

[p-minInstancesPerNode]#Minimum instances per node#::
For a node to be split further, the split must improve at least this much
(in terms of information gain).

[p-seed]#Seed#::
We maximize the information gain only among a subset of the possible splits.
This random seed is used for selecting the set of splits we consider at a node.
====
