## Tables

Tables represent simple record-oriented data in LynxKite, very similar to tables in relational
databases. Like database tables, LynxKite tables can represent various data types. They can be of
any size, however, as they are stored in an efficient columnar format on the distributed file
system.

Tables can be imported from external data sources like files or databases, or they can be created
from SQL queries on graphs. Tables just contain data. They cannot be modified and will not change
even if the imported file is deleted or the source project is updated. For a quick overview of
its data, click on the name of the table. Otherwise, a table has to be imported into a project to
start working with its data. See the import operations on projects for more details.

Access the dropdown menu for a table in the project browser
(+++<a href class="btn-dropdown dropdown-toggle" dropdown-toggle><span class="caret"></span></a>+++)
to discard, duplicate, or rename the table. The rename command also makes it possible to move the
table to a different path. +++ <i class="glyphicon glyphicon-adjust"></i> Edit import +++ loads the
configuration into the import wizard that was used to create the table.

The import operations also allow creating tables, but a table can also be created from the project
browser.

Click +++
<span class="project-list" style="display: inline-block;">
  <span class="entry" style="display: block;">
    <span style="display: block;" class="icon glyphicon glyphicon-plus"></span>
    <span class="lead">Import table</span>
  </span>
</span>
+++ in the project browser to import a table inside the current folder. The same interface for
importing data into a table is also available in the import operations via the
+++<label class="btn btn-default"><i class="glyphicon glyphicon-import"></i></label>+++
button. In this case the table is created in the root folder by default. To put the table
somewhere else you have to specify its full path, e.g. `Users/MyUser/MyTable`.


[[limit]]
### Limit

It is possible to import a limited number of rows only.
This is useful when working with large data sets.
Limit the import to do some quick experiments on small data, then edit your table/view and reload
it in the <<project-history, history editor>> to rerun all calculations on the full data set.

[[import-formats]]
### Supported data formats

The supported table input formats and their respective options are described in the following
sections. LynxKite fully interoperates with all of these formats. (Both import and export are
supported.)

[[import-csv]]
#### CSV

CSV stands for comma-separated values. It is a common human-readable file format where each record
is on a separate line and fields of the record are simply separated with a comma or other delimiter.
CSV does not store data types, so all fields become strings when importing from this format.

====
[[path]] Path::
Upload a file by clicking the
+++<label class="btn btn-default"><i class="glyphicon glyphicon-cloud-upload"></i></label>+++ button
or specify a path explicitly. Wildcard (`+foo/*.csv+`) and glob (`+foo/{bar,baz}.csv+`) patterns are
accepted. See <<prefixed-paths>> for more details on specifying paths.

[[columns]] CSV columns::
The names of all the columns in the file, as a comma-separated list. If empty, the column names will
be read from the file. (Use this if the file has a header.)

[[delimiter]] Delimiter::
The delimiter separating the fields in each line.

[[mode]] Format error handling::
What should happen if a line has more or less fields than the number of columns?
+
**Fail on any malformed line** will cause the import to fail if there is such a line.
+
**Ignore malformed lines** will simply omit such lines from the table. In this case an erroneously
defined column list can result in an empty table.
+
**Salvage malformed lines: truncate or fill with nulls** will still import the problematic lines,
dropping some data or inserting undefined values.

[[infer]] Infer types::
Automatically detects data types in the CSV. For example a column full of numbers will become a
`Double`. If disabled, all columns are imported as ``String``s.
====

[[import-json]]
#### JSON

JSON is a rich human-readable data format. JSON files are larger than CSV files but can represent
data types. Each line of the file in this format stores one record encoded as a
https://en.wikipedia.org/wiki/JSON[JSON] object.

====
[[path]] Path::
Upload a file by clicking the
+++<label class="btn btn-default"><i class="glyphicon glyphicon-cloud-upload"></i></label>+++ button
or specify a path explicitly. Wildcard (`+foo/*.json+`) and glob (`+foo/{bar,baz}.json+`) patterns
are accepted. See <<prefixed-paths>> for more details on specifying paths.
====

[[import-parquet]]
#### Apache Parquet

https://parquet.apache.org/[Apache Parquet] is a columnar data storage format.

====
[[path]] Path::
The distributed file-system path of the file. See <<prefixed-paths>> for more details on specifying
paths.
====

[[import-orc]]
#### Apache ORC

https://orc.apache.org/[Apache ORC] is a columnar data storage format.

====
[[path]] Path::
The distributed file-system path of the file. See <<prefixed-paths>> for more details on specifying
paths.
====

[[import-jdbc]]
#### JDBC

JDBC is used to connect to relational databases such as MySQL. See <<jdbc-details>> for setup steps
required for connecting to a database.

====
[[url]] JDBC URL::
The connection URL for the database. This typically includes the username and password. The exact
syntax entirely depends on the database type. Please consult the documentation of the database.

[[table]] Table::
The name of the database table to import.
+
All identifiers have to be properly quoted according to the SQL syntax of the source database.
+
The following formats may work depending on the type of the source database:
+
* `TABLE_NAME`
* `SCHEMA_NAME.TABLE_NAME`
* `(SELECT * FROM TABLE_NAME) TABLE_ALIAS`

[[key-column]] Column used for data partitioning::
This column is used to partition the SQL query. The range from `min(key)` to `max(key)`
will be split into a sub-range for each Spark worker, so they can each query a part of the data in
parallel.
+
Pick a column that is uniformly distributed. Numerical identifiers will give the best performance.
String (`VARCHAR`) columns are also supported but only work well if they mostly contain letters of
the English alphabet and numbers.
+
If the partitioning column is left empty, only a fraction of the cluster resources will be used.
+
The column name has to be properly quoted according to the SQL syntax of the source database.
====

[[import-hive]]
#### Apache Hive

Import an https://hive.apache.org/[Apache Hive] table directly to LynxKite.

====
[[table-name]] Hive table::
The name of the Hive table to import.
====
