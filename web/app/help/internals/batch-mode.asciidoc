[[batch-mode]]
## Batch processing API

You can do programmatic graph manipulation in LynxKite using batch scripts.

The first option to do that is to start LynxKite in *batch mode* (instead of the normal way,
as a long-running web server that offers an interactive user interface).
In this mode LynxKite executes a script and terminates when it is finished. This is useful for running periodic
tasks without holding on to resources continuously.

To run Kite in batch mode, do:

[subs=normal]
 ./run-kite.sh batch _script_file_ [_parameters_]

Where `_parameters_` is a list of `key:value` pairs.

The same scripts that can be run in batch mode can also be run on a Kite instance in interactive mode. To
do that, login with the Ammonite ssh interface (see kiterc for docs) and run the following command:

 batch.runScript("name_of_script_file", "param1" -> "value1", "param2" -> "value2", ...)

Whichever way you use to run a batch script, the script and the web interface operate on the
same data. So a batch script could apply changes to a graph created in interactive mode, or
conversely interactive mode could be used to explore a graph created in batch mode, simply by
using the same project names.


### Script file syntax

Batch script files are written in the http://www.groovy-lang.org/[Groovy] programming language.

The script will receive the command line arguments in the `params` map that is injected into its
namespace.

The script can interact with LynxKite through the `lynx` object that is injected into its
namespace.

 - `GroovyProject **lynx.loadProject**(String name)` loads an existing project. Use
   `GroovyProject.saveAs()` if you want to save your changes.
 - `GroovyProject **lynx.newProject**()` creates a new project.
 - `DataFrameReader **lynx.sqlContext**` provides access to Spark SQL.
   See the
   http://spark.apache.org/docs/latest/sql-programming-guide.html[Spark SQL and DataFrame Guide]
   to learn more. (The Java examples also apply to Groovy.)
 - `DataFrame **lynx.sql**(String query)` is a shortcut to `lynx.sqlContext.sql`. It executes a
   Spark SQL query and returns the result as an Apache Spark DataFrame.
 - `String **lynx.saveAsTable**(DataFrame df, String tableName, String notes = "")` saves an
   Apache Spark DataFrame as a LynxKite table. It returns the table identifier that can be used
   as a parameter for import operations.
 - `String **lynx.openTable(String tableName)` gets the table identifier of a table known
   by its name. This identifier can be used as a parameter for import operations.
 - `String **lynx.resolvePath**(String path)` resolves a <<prefixed-paths, prefixed path>>.
 - `StructType **lynx.stringSchema**(List[String] columns)` creates a schema with `StringType` columns
   It can be used when loading tables via the `sqlContext`.

The properties of `GroovyProject` are:

 - `Map<String, GroovyProject> **segmentations**` is the list of segmentations for the project.
 - `Map<String, GroovyScalar> **scalars**` is the list of scalars. `GroovyScalar` has a `toString()`
   and a `toDouble()` method, they return its value as a String/Double respectively.
 - `Map<String, GroovyAttribute> **vertexAttributes**` and
   `Map<String, GroovyAttribute> **edgeAttributes**` are the vertex and edge attributes.
   `GroovyAttribute` has a `histogram()` method that
   returns a histogram of the attribute with 10 buckets. You can also call it with additional
   optional arguments, for example: `histogram(numBuckets: 12, logarithmic: true, precise: false)`.
 - `DataFrame **vertexDF**` gives you access to the vertex attributes of the project
   as an Apache Spark DataFrame. `DataFrame **edgeAttributeDF**` does the same for the edges.
   `DataFrame **edgeDF**` joins the two tables, giving access to the vertex attributes with
   `src_` and `dst_` prefixes and to the edge attributes with the `edge_` prefix.
   `DataFrame **belongsToDF**` (defined on segmentations)
   does it for the links going between a project and its segmentation with vertex attributes
   prefixed with `base_` and `segment_`. See the
   http://spark.apache.org/docs/latest/sql-programming-guide.html[Spark SQL and DataFrame Guide]
   to learn about working with DataFrames.

`GroovyProject` has a method for every operation. These methods take named arguments for each
operation parameter. Rather than documenting these methods in detail, the recommended approach is
to run the operations in interactive mode, enter the <<project-history, history editor>> and the
<<saving-a-workflow, workflow editor>> within. The correct Groovy code can then be copied from the
workflow editor.

Additional methods of `GroovyProject` are:

 - Workflows can be run with the `**runWorkflow**()` method. Workflows are internally
   identified by their creation timestamps in addition to their names. The first argument of
   `runWorkflow` is the workflow name either with or without the timestamp. If the timestamp is
   omitted, the latest version of the workflow is used. The workflow parameters can be passed as
   named arguments to `runWorkflow`, same as other operations.
 - `**saveAs**(String name)` can be used to save a project into LynxKite. Until `saveAs` is called,
   the persistent state in LynxKite is not affected by the batch script.
 - `**copy**()` returns a copy of the `GroovyProject` object that can be manipulated independently
   from the original.

### Full example

In this example we create a script that reads edges from a CSV file, calculates PageRank, exports
PageRank to another CSV, and prints the number of vertices and the time the whole script took.

----
start_time = System.currentTimeMillis()
// Import input, calculate PageRank, export output.
project = lynx.newProject()
df = lynx.sqlContext.read()
  .format('com.databricks.spark.csv')
  .option('header', 'true')
  .load(params['input'])
// Alternatively if there are no headers in the CSV file.
df = lynx.sqlContext.read()
  .format('com.databricks.spark.csv')
  .option('header', 'false')
  .schema(lynx.stringSchema(['src', 'dst']))
  .load(params['input'])
project.importVerticesAndEdgesFromASingleTable(
  table: lynx.saveAsTable(df, 'Batch Example Table'),
  src: 'src',
  dst: 'dst')
project.computePageRank(
  name: 'page_rank',
  iterations: '5',
  damping: '0.85')
project.exportVertexAttributesToFile(
  path: params['output'],
  link: 'exported_csv',
  attrs: 'stringID,page_rank',
  format: 'CSV')

// Print metrics.
count = project.scalars['!vertex_count']
time = (System.currentTimeMillis() - start_time) / 1000
println "$count vertices processed in $time seconds."

// Example of Spark SQL integration.
project.vertexDF.printSchema()
project.vertexDF.groupBy('gender').count().show()
df = project.sql('select * from vertices where gender = "Male"')
df.show()
df.write().format('com.databricks.spark.csv').option('header', 'true').save('males.csv')
df.write().format('parquet').save('males.parquet')
df.write().format('json').save('males.json')

// Save the project into LynxKite so it can be explored via the web interface.
project.saveAs('Batch PageRank')
----

When running the script we must use <<prefixed-paths>> for the file names. For example:

 ./run-kite.sh batch pagerank.groovy input:UPLOAD$/data-2015.csv output:UPLOAD$/pagerank-2015.csv

See the `kitescripts` directory in the LynxKite installation for more complex example scripts.
