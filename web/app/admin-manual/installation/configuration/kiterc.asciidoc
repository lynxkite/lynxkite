[[kiterc-file]]
### The `.kiterc` file

This file contains various configurations used when launching a LynxKite instance.

[[kite-instance]]
Specify this to be the instance (client) where the program is run. E.g., MyCLient.
```
export KITE_INSTANCE=
```

Specify the directory containing your Spark installation.
```
export SPARK_HOME=$HOME/spark-${SPARK_VERSION}
```

To specify which cluster to use choose one of the following options:

- `local` - for single machine installs
- `yarn` - for using YARN (Hadoop v2). Please also set `YARN_*` settings below.

```
export SPARK_MASTER=local
```

Specify the directory where LynxKite stores metadata about projects. The directory must be on
the local file system. Do not forget to set up backups.
```
export KITE_META_DIR=$HOME/kite_meta
```

Specify the directory of the data where graph data is stored. Should be on a distributed fs
(HDFS, S3, etc) unless this is a `local` setup. Please note that giving the full
URI (e.g., `file:/...`) is mandatory.
```
export KITE_DATA_DIR=file:$HOME/kite_data/
```

LynxKite can use a faster ephemeral file system for data storage, with reading from the main
file system specified by `KITE_DATA_DIR` as a fallback. The primary use case for this is using
SSD-backed HDFS on EC2 while using S3 for the permanent storage.

If `KITE_EPHEMERAL_DATA_DIR` is set, all data writes go to this directory. `KITE_DATA_DIR will` be
used only for uploaded files.
```
# export KITE_EPHEMERAL_DATA_DIR=
```

Optionally set the configuration file to specify what file paths users are allowed to access using
what prefixes. Use `kite_x.y.z/conf/prefix_definitions_template.txt` as a template when creating
your custom prefix configuration file. Make sure you put the config file to a LynxKite version
independent location (not inside the `kite_x.y.z` directory, e.g. `$HOME/kite_conf`). Then set
`KITE_PREFIX_DEFINITIONS` to point to your newly created config file (e.g. the recommended
`$HOME/kite_conf/prefix_definitions.txt`).
```
export KITE_PREFIX_DEFINITIONS=
```

Normally, LynxKite can only access the file system via its prefix mechanism,
that is, using either the `DATA$` prefix (as in `DATA$/dir/file.csv`) or any other
prefixes set up in a prefix definition file. This behavior can be changed by
setting `KITE_ALLOW_NON_PREFIXED_PATHS` to `true`: then it becomes possible to
specify any valid paths such as `file:/home/user/kite_data/dir/data.csv`
or `s3n://awskey:awspassword@somedir/somefile`. Be careful: all LynxKite users will
be able to read and write local and cluster files with the credentials of the LynxKite process.
Only allow this in restricted environments.
```
export KITE_ALLOW_NON_PREFIXED_PATHS=false
```

Specify the PID file for LynxKite servers.
```
export KITE_PID_FILE=$HOME/kite.pid
```

When set, each action performed by the lynxkite script (start,stop,restart, etc.) is logged to
this file.
```
export KITE_SCRIPT_LOGS=$HOME/lynxkite.log
```

Specify the user registry file for LynxKite servers.
```
export KITE_USERS_FILE=$HOME/kite_users
```

Specify the YARN configuration directory. It is needed if you want to run against YARN.
```
# export YARN_CONF_DIR=/etc/hadoop/...
```

By default, LynxKite specifies 15% of executor memory to be the amount allocated for overhead
in YARN executor containers. You can set a higher value if YARN is killing your executors for
exceeding size, but the executors themselves are not reporting out of memory errors.
```
# export YARN_EXECUTOR_MEMORY_OVERHEAD_MB=4000
```

YARN offers the possibility of using resource pools. You can specify an existing resource pool and
allocate LynxKite to it.
```
# export RESOURCE_POOL=
```

Specify how much memory is available for LynxKite on a single worker machine.
Ignored for `local` setups, use `KITE_MASTER_MEMORY_MB` for that case.
```
export EXECUTOR_MEMORY=1g
```

Specify the number of executors. For standalone cluster it defaults to as many as possible.
For a YARN setup, this options is mandatory.
```
# export NUM_EXECUTORS=5
```

Specify the number of cores per executor LynxKite should use. For local mode, you
can set it to `*` to use as many cores as available.
```
export NUM_CORES_PER_EXECUTOR=4
```

Specify how much memory is available for LynxKite on the master machine in megabytes.
For `local` setups this also determines executor memory and `EXECUTOR_MEMORY` is
ignored in that case.
```
export KITE_MASTER_MEMORY_MB=1024
```

Specify the port for the LynxKite HTTP server to listen on. Must be >=1000.
```
export KITE_HTTP_PORT=2200
```

By default, LynxKite can only be accessed from localhost for security reasons. (That is,
address `127.0.0.1`). Uncomment the following line if LynxKite should be accessible
from other IP addresses as well.
```
# export KITE_HTTP_ADDRESS=0.0.0.0
```

Specify the HTTP port for the watchdog. If this is set, the startup script will start a watchdog
as well which will automatically restart the LynxKite server if it detects any problem.
```
# export KITE_WATCHDOG_PORT=2202
```

Uncomment this to start an internal watchdog thread inside LynxKite's
driver JVM. This watchdog will kill LynxKite if health checks are
failing continuously for the given amount of time.
```
# export KITE_INTERNAL_WATCHDOG_TIMEOUT_SECONDS=1200
```

The LynxKite local temp directory is a local path that exists on all workers and the master and will
be used for storing temporary Spark/Hadoop files. This directory can potentially use a lot of
space, so if you have a small root filesystem and an extra large drive mounted somewhere then you
need to point this to somewhere on the large drive. On the other hand performance of this drive has
a significant effect on overall speed, so using an SSD is a nice option here.
```
export KITE_LOCAL_TMP=/tmp
```

[[kiterc-logging]]
Configure the directory LynxKite uses for logging. Defaults to `logs` under the LynxKite
installation directory if not specified.
```
# export KITE_LOG_DIR=
```

[[kiterc-extra-jars]]
The LynxKite extra JARS is a colon (:) delimited list of JAR files that should be loaded on the
LynxKite `CLASSPATH`. (It will be loaded on the master and distributed to the workers.)

- Some wildcards are supported, you can use `/dir/*`, but you cannot use `/dir/*.jar`.
- Filenames have to be absolute paths.

One typical use case is to configure additional JDBC drivers. To do that, all you need to do is to
add the jar file here.
```
export KITE_EXTRA_JARS=
```

You can enable an interactive Scala interpreter able to access LynxKite internals by using
the below exports. You can access the interpreter by SSHing from the host running LynxKite as:
`ssh ${KITE_AMMONITE_USER}@localhost -p ${KITE_AMMONITE_PORT}`
and use `KITE_AMMONITE_PASSWD` as password.

If you do this and do not trust all users who can SSH into this machine (the typical case!)
then make sure to modify the file system permissions of `.kiterc` to be only readable by the
user running LynxKite and change the password below.
```
# export KITE_AMMONITE_PORT=2203
# export KITE_AMMONITE_USER=lynx
# export KITE_AMMONITE_PASSWD=kite
```

[[kiterc-https]]
Options needed if you want to use authentication and HTTPS.

Just use the following configurations with default values for a simple, fake certificate setup.

===========================================================
Application secret used by Play! framework for various tasks, such as signing cookies and
encryption. Setting this to `<random>` will regenerate a secret key at each restart.
More details can be found
https://playframework.com/documentation/latest/ApplicationSecret[here].
```
# export KITE_APPLICATION_SECRET='<random>'
```

Specify the port for the LynxKite HTTPS server to listen on. Must be >=1000.
```
# export KITE_HTTPS_PORT=2201
```

By default if you turn on HTTPS, only logged-in users will be able to access the LynxKite UI.
If you want users to be able to have read-only access without logging in, set
`KITE_ACCESS_WITHOUT_LOGIN` to `yes`. Whether these users are restricted to using wizards
is controlled by the second setting.
```
# export KITE_ACCESS_WITHOUT_LOGIN=yes
# export KITE_WIZARD_ONLY_WITHOUT_LOGIN=yes
```

Set the keystore file and password with the HTTPS keys. Use the default values for a fake HTTPS
certificate. If you have your own intranet CA or a wildcard certificate, you can generate a
certificate for LynxKite that the browsers can validate. Follow the instructions at
http://tomcat.apache.org/tomcat-6.0-doc/ssl-howto.html[Apache Tomcat] for creating a keystore file.
```
# export KITE_HTTPS_KEYSTORE=${KITE_DEPLOYMENT_CONFIG_DIR}/localhost.self-signed.cert
# export KITE_HTTPS_KEYSTORE_PWD=keystore-password
```
===========================================================

On a Kerberos-secured Hadoop cluster, set the KERBEROS_PRINCIPAL and KERBEROS_KEYTAB
variables. The principal acts like a user name and the keytab file acts like a password.
```
# export KERBEROS_PRINCIPAL=
# export KERBEROS_KEYTAB=
```

The setting `KITE_MAX_ALLOWED_FILESYSTEM_LIFESPAN_MS` forces LynxKite to create a
new Hadoop Filesystem object every time `KITE_MAX_ALLOWED_FILESYSTEM_LIFESPAN_MS` milliseconds
have passed. This mechanism prevents certain errors related to Hadoop delegation token expiration.
A good value for this is half the time specified in the system's
`dfs.namenode.delegation.token.renew-interval` setting. That defaults to 1 day, so our default is 12 hours.
See https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained[this explanation]
for details.
```
# export KITE_MAX_ALLOWED_FILESYSTEM_LIFESPAN_MS = $((1000*60*60*12))
```

Uncomment the below lines to export LynxKite's Spark metrics into
a Graphite-compatible monitoring system. You can use this together
with `tools/monitoring/restart_monitoring_master.sh`.
```
# export GRAPHITE_MONITORING_HOST=$(hostname)
# export GRAPHITE_MONITORING_PORT=9109
```

Specify any command line arguments for the jvm that runs the LynxKite driver.
(e.g., EXTRA_DRIVER_OPTIONS='-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=50000'
will run a local LynxKite in a such a mode that you can attach to it with a debugger.)
```
# export EXTRA_DRIVER_OPTIONS=
```

Set thread stack size for thread in the executor process. By default, jvm will assign
1 megabyte for each thread, but that is too low for some rdd dependencies.
The suffixes M and k can be used to specify megabytes or kilobytes respectively; e.g., 1M or 1500k.
This setting does not have an effect in local mode.
```
# export EXECUTOR_THREAD_STACK_SIZE=3M
```

Set thread stack size for driver threads. By default, jvm will assign 1 megabyte for each thread,
but that is too low for some rdd dependencies. The suffixes M and k can be used to specify megabytes
or kilobytes respectively; e.g., 1M or 1500k.
```
# export DRIVER_THREAD_STACK_SIZE=2M
```

Maximize the number of parallel threads. LynxKite, Spark domain and Sphynx will
all use this parameter.
```
# export KITE_PARALLELISM=5
```

Specify the length of the protected time period in days, while the cleaner does not
delete data files. It can be integer or double value.
```
# export KITE_CLEANER_MIN_AGE_DAYS=14
```

The ecosystem docker image comes with JupyterLab, which can be used to run
LK Python API code. The default port of JupyterLab is 8888, but it can be changed.
```
# export KITE_JUPYTER_PORT=9999
```

Specify where Sphynx (the single-node server) is running and where the certificate can be found.
If any of these is unset, no Sphynx server is assumed.
```
export SPHYNX_HOST=localhost
export SPHYNX_PORT=50051
export SPHYNX_CERT_DIR=$HOME/sphynx_cert
```
Specify the PID file for Sphynx.
```
export SPHYNX_PID_FILE=$HOME/sphynx.pid
```
Specify the directory where Sphynx stores graph data.
```
export ORDERED_SPHYNX_DATA_DIR=$HOME/ordered_sphynx_data
```
Specify the directory where Sphynx stores graph data using vertex ids from the Spark world.
```
export UNORDERED_SPHYNX_DATA_DIR=$HOME/unordered_sphynx_data
```

Unrestricted Python in LynxKite is a very powerful tool. Since Python execution has full access
to the network and file-system, it must be explicitly enabled by the administrator.
```
# export KITE_ALLOW_PYTHON=yes
```

A simple chroot-based sandbox for executing users' Python code can improve security in a multi-user
environment. To be able to mount directories as read-only, this requires running LynxKite as root.
If it's running in a Docker container, that container must be started with the `--privileged` flag.
```
# export SPHYNX_CHROOT_PYTHON=yes
```

Sphynx can keep entities in memory for high-performance computation. This setting
configures how much memory to allocate for this purpose.
```
export SPHYNX_CACHED_ENTITIES_MAX_MEM_MB=2000
```
