## Configuring LynxKite

### The `.kiterc` file

You need to create a site configuration to be able to run LynxKite. If you don’t have a config file
created yet, running the runner script will warn you and point you to a template config file. Just
copy that template to your home directory with name `.kiterc` and edit the settings as necessary.
The settings are documented inside the template file and in the <<kiterc-file>> section. This
information is also available in the template file as comments.

[[configure-directories]]
### Configure directories

This is a best practices section on how to configure various directories properly. The default
configuration provided by Hadoop may be suboptimal.

- DataNode directories should be on a large drive. Set `dfs.data.dir` and `dfs.datanode.data.dir`
  on every DataNode accordingly.
- NameNode and DataNode directories are better to not exist or at least be empty before you start
  the HDFS service the first time / deploy a new node. Check `dfs.name.dir`,
  `dfs.namenode.name.dir`, `dfs.data.dir` and `dfs.datanode.data.dir`.
- Datanode directories have to be owned by `hdfs:hadoop`.
- YARN log dirs should be on a large drive. Check `yarn.nodemanager.log-dirs` and `hadoop.log.dir`
  in the NodeManager and `hadoop.log.dir` in the ResourceManager configuration.
- YARN log dir has to be owned by `yarn:hadoop`
- Set the `yarn.nodemanager.local-dirs` (YARN app cache and file cache) to be on the appropriate
  drive. It should be sufficiently large (typically at least 100GB).
- Make sure that you have properly configured prefixes if you need them as described in the
  <<prefix-definitions>> section.

### Running LynxKite

LynxKite can be started with the `lynxkite` command in the `bin` directory of the LynxKite installation.
For example: `kite_2.6.2/bin/lynxkite start`. Use the
`start` / `stop` / `restart` options if you want to run it as a daemon. Or, you can use the
`interactive` command option, which starts up the program tied to your current shell session, so its output
will appear in the shell and pressing Ctrl-C or closing the shell session will terminate the
program.

### User authentication

In the default configuration user authentication is turned off. To enable user authentication,
you have to enable HTTPS (see next section). To securely set up user authentication:

- Make sure LynxKite is not accessible externally.
- Start LynxKite without authentication. Go to http://<lynxkite_url>/#/users and add at
  least one new administrator user. (Administrator users can create more users.)
- Enable HTTPS in the `.kiterc` file.
- Restart LynxKite with authentication. Confirm that user credentials are now required for login.
- You can now make LynxKite externally accessible.

In addition to this, LynxKite can be authenticated using a remote LDAP service. LDAP authenticated
users will always be non-administrators. Regular LynxKite and LDAP user authentication can coexist.
The details of setting up LDAP authentication are specified in the
<<ldap-authentication,LDAP authentication section>> of the `.kiterc` chapter.

### Access control setup

After the users are created, HDFS/Linux directory level access control can be enabled
as explained in <<prefix-based-access-control>>. LynxKite level access control can be managed
directly on the LynxKite Web UI.

### HTTPS setup

To enable HTTPS, a certificate is required. For testing purposes a self-signed certificate is
included (`conf/localhost.self-signed.cert`). Its password is “keystore-password”. This certificate does
not provide proper protection. (Lacking a trusted CA, the self-signed certificate can easily
be spoofed.) Using it will trigger a certificate warning in the user’s browser. It is recommended
to install a real certificate in its place.

Please refer to the <<kiterc-https,HTTPS setup>> in the `.kiterc` file for more details.

### Backups

Once LynxKite is up and running, take a moment to think about backups. The solution is entirely
dependent on the site and purpose of the installation. But consider that LynxKite deals with the
following data:

- *Import/export.*
- *Graph data in `KITE_DATA_DIR`.* The vertices, edges and attributes of the graph. This data
  is stored on a distributed file system. This typically provides resilience against data loss.
- *Project metadata in `KITE_META_DIR`.* The operations that have been executed on the project
  and their parameters. This data is small, and contains everything required to regenerate the
  graph data, assuming that the imported files are still available. This data is critical, and
  backups are recommended.

There is a backup tool for Hadoop deployments shipped with LynxKite under
`tools/kite_meta_hdfs_backup.sh`. It is recommended to create a cron job running this tool on
a daily basis (or more frequently if needed). The tool creates a copy of the full `KITE_META_DIR`
and the `.kiterc` file in HDFS.

To restore an earlier version of LynxKite simply copy the `.kiterc` file and the meta
directory back from HDFS to overwrite the current versions of them, then restart LynxKite.


### Extra libraries

You can provide extra jar files that will be added to the `CLASSPATH` of LynxKite server if needed,
as specified <<kiterc-extra-jars,here>>.

### Integration with Hive

#### Overview
Hive stores the actual data on HDFS (usually under the `/user/hive/warehouse` directory), while the
metadata (location of the underlying file, schema, ...) of the tables is stored by Hive Metastore.
When Spark wants to import a Hive table then it first asks the Hive Metastore for the metadata then
tries to import directly from the underlying files.
This means that for Spark to import a Hive table, it needs:

 - to know how to access the Hive Metastore;
 - to have the authorization to access the Hive Metastore;
 - to have the authorization to read the metadata of the table;
 - to have read access for the underlying file on HDFS.

#### Telling Spark how to access the Hive Metastore
The simplest way to do this is to create a symbolic link from the `conf` directory under the Spark
installation directory LynxKite is using to the `hive-site.xml` file.
E.g. depending on your Hadoop distribution, something like this would do:

```
ln -s /etc/hive/conf/hive-site.xml ~/spark-<version>/conf/
```

#### Getting authorization to read the metadata of a table
Hive Metastore server supports Kerberos authentication for Thrift clients, so it is possible that
on clusters using Kerberos the access to Hive Metastore is restricted to only a subset of Kerberos
users.

To see if that is the case on your cluster, check the `hadoop.proxyuser.hive.groups` property in
the `core-site.xml` on the Hive Metastore host. If its value is set to `*` or it is not set then
that means that there is no restriction on who can acces the Hive Metastore. However, if it
contains a list of groups, then add the group of the user running LynxKite to the list.

For Cloudera-specific details, see the Cloudera 5.6 documentation on
https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_sg_hive_metastore_security.html[
Hive Metastore Security].

If the impersonating concept is not clear from the above article, then the general idea is better
explained
https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html[here].

#### Getting authorization to read the metadata of given table
Hive offers 3 https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization[
authorization options].

- https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Server[
Storage Based Authorization in the Metastore Server]: this means that the user has the same access
right for the metadata as he has for the underlying data on HDFS.


- https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization[SQL
 Standards Based Authorization]: Like in a MySQL database you add privileges to users.


- https://cwiki.apache.org/confluence/display/Hive/Hive+Default+Authorization+-+Legacy+Mode[
Default Hive Authorization (Legacy Mode)]: Uses roles to group grants. These roles then can be
assigned to users, groups or to other roles.


#### Getting authorization to read the underlying file on HDFS
Please consult with the owner of the cluster to give you read access for all the corresponding files.

#### Known issues

With some Hive configurations, LynxKite cannot import tables due to a missing jar file. We cannot
distribute that jar file (com.hadoop.gplcompression.hadoop-lzo-0.4.17.jar) due to licensing reasons.
You should download it yourself and put it in the directory specified by `KITE_EXTRA_JARS` (see <<kiterc-file>>).

Sometimes the values in an imported table all become nulls. This is a case-sensitivity issue.
Hive is case insensitive when it comes to table names and column names. Because of this, Hive
Metastore stores the column names in all lower case. But Parquet files and ORC are case preserving.
This can result in a situation where a column name is stored in all lower-case in Hive Metastore
but the same column name in the underlying files is stored using upper-case letters. Spark imports
data directly from the underlying files but uses the column names stored in Hive Metastore to look
for the columns it needs to import.
Thus if the underlying files have a column name containing upper-case letters then Spark tries to
import data from a non-existing column (since the column name is written differently in the
underlying files than in Hive Metastore) and this results in columns with all `nulls`.

One exception is the partitioner column - since the partitioner is part of the HDFS path, Hive
Metastore is forced to store it in a case preserving manner.
So if you can change the schema of the underlying files without causing problem then that is the
fix. If not, then create new tables whose underlying files' column names are all in lower
case. For example, the following query can be used:
`CREATE TABLE <new_table> AS SELECT * FROM <old table>`.

### Integrating with High Availability mode HDFS

LynxKite is compatible with HDFS running in High Availability mode. In this case the HDFS prefixes
i.e. the `KITE_DATA_DIR` variable in <<kiterc-file>> and the <<prefix-definitions>> need to use the
appropriate name service defined in the `hdfs-site.xml` configuration file of Hadoop (e.g.
`hdfs://nameservice1/user/my_user/my_dir`). Make sure that the file is available in the
`YARN_CONF_DIR`. Please refer to the related
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html[Hadoop documentation]
for more details.
